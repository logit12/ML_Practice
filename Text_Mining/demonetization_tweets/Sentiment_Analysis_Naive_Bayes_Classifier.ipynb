{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "    \n",
    "We are going to implement an algorithm to classify reviews depending on the opinion polarity\n",
    "This processing is also called “sentiment analysis”. The main algorithm used here is the **Naive Bayes**.\n",
    "\n",
    "**We will implement 2 versions of Naive Bayes : A scratch version and another from with scikit-learn and NLTK**\n",
    "\n",
    "For this exercice, we use 2 set of files. One represents negative opinion and the other one represents positive opinion. \n",
    "\n",
    "## Data Source : KAGGLE\n",
    "(search demonetization-tweets dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "## From Scratch Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews: 5331\n",
      "\n",
      "A Sample of positive review:\n",
      " the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      "Number of negative reviews: 5331\n",
      "\n",
      "A Sample of negative review:\n",
      " simplistic , silly and tedious . \n"
     ]
    }
   ],
   "source": [
    "enc='latin-1'\n",
    "\n",
    "# load the tweets\n",
    "df_tweets = pd.read_csv('../sentimentdata/demonetization-tweets.csv', encoding=enc)\n",
    "\n",
    "# load the words weight \n",
    "f = open('../sentimentdata/AFINN-111.txt', 'r')\n",
    "word, weight = [], []\n",
    "for line in f.readlines():\n",
    "    word.append(line.split()[0])\n",
    "    weight.append(line.split()[1])\n",
    "\n",
    "df_words_weight = pd.DataFrame.from_dict({'word':word, 'weight':weight})\n",
    "\n",
    "# load the reviews with trailing whitespace removed \n",
    "positive_reviews = [line.rstrip('\\n') for line in open('../sentimentdata/rt-polarity.pos', encoding=enc)]\n",
    "negative_reviews = [line.rstrip('\\n') for line in open('../sentimentdata/rt-polarity.neg', encoding=enc)]\n",
    "\n",
    "print('Number of positive reviews:', len(positive_reviews))\n",
    "print('\\nA Sample of positive review:\\n', positive_reviews[0])\n",
    "print('\\nNumber of negative reviews:', len(negative_reviews))\n",
    "print('\\nA Sample of negative review:\\n', negative_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After loading data set, we are going to clean them by removing punctuation and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean_text(input_reviews):\n",
    "    \"\"\"Clean a list of reviews: return text cleaned, without punctuation, \n",
    "    special caracters and a set of stop words in english\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_reviews: list. \n",
    "        list of text\n",
    "     \n",
    "    Returns\n",
    "    -------\n",
    "    cleaned_reviews: list. \n",
    "        List of cleaned text\n",
    "    \"\"\"\n",
    "    cleaned_reviews = []\n",
    "    # remove punctuation\n",
    "    regex_no_punct = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    reviews = [regex_no_punct.sub('', each_review) for each_review in input_reviews]\n",
    "   \n",
    "    # load and process stop words\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    for each_review in reviews:\n",
    "        new_text=[]\n",
    "        for word in each_review.split():\n",
    "            if word not in stopWords:\n",
    "                new_text.append(word)\n",
    "        cleaned_reviews.append(new_text)\n",
    "            \n",
    "    return cleaned_reviews\n",
    "\n",
    "positive_reviews_c = clean_text(positive_reviews)\n",
    "negative_reviews_c = clean_text(negative_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we would like to predict for the reviews which one is positive or negative.\n",
    "\n",
    "As we do not have a test set, we will prepare a training and a test set.\n",
    "\n",
    "First, we produce the classification target for these documents\n",
    "The negative opinion will be represented by class 0 and the positive opinion will be represented by class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10662\n"
     ]
    }
   ],
   "source": [
    "all_reviews = negative_reviews_c + positive_reviews_c\n",
    "print(len(all_reviews))\n",
    "y = np.ones(len(all_reviews), dtype=np.int)\n",
    "y[:len(negative_reviews_c)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we count the occurences number of each word in the reviews and we build a vocabulary.  For that, i define a function count_words as follow. It takes few minutes, don't worry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples = 10662, n_features = 20360\n",
      "CPU times: user 1min 55s, sys: 2.19 s, total: 1min 58s\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def count_words(input_reviews):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_reviews : list of str\n",
    "        The texts\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict_words_and_count : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "        n_samples == number of documents.\n",
    "        n_features == number of words in vocabulary.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the number of occurences of each word in the whole reviews \n",
    "    dict_words_and_count = Counter(all_reviews[0])\n",
    "    for each_review in all_reviews[1:]:\n",
    "        dict_words_and_count += Counter(each_review)\n",
    "\n",
    "    # get all the words that build the vocabulary\n",
    "    vocabulary = dict_words_and_count.keys()\n",
    "\n",
    "    # the words are the features and the reviews are the samples\n",
    "    # get their size\n",
    "    n_samples, n_features = len(all_reviews), len(vocabulary)\n",
    "    \n",
    "    counts = np.zeros(shape=(n_samples, n_features), dtype=int)\n",
    "\n",
    "    for j, each_review in enumerate(all_reviews):\n",
    "        for i, word in enumerate(vocabulary):\n",
    "            counts[j, i] = each_review.count(word)\n",
    "    print(\"n_samples = %d, n_features = %d\"%( n_samples, n_features))\n",
    "    return dict_words_and_count, counts\n",
    "\n",
    "\n",
    "# Count words in text\n",
    "dict_words_and_count , X = count_words(all_reviews) \n",
    "VOCABULARY=list(dict_words_and_count.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " We split our dataset in training and testing set and we verify that they are quite balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4344 4318\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(29)\n",
    "rand_index = np.random.permutation(X.shape[0])\n",
    "\n",
    "X_test, X_train = X[rand_index[:2000]], X[rand_index[2000:]]\n",
    "y_test, y_train = y[rand_index[:2000]], y[rand_index[2000:]]\n",
    "\n",
    "print(len(y_train[y_train==0]), len(y_train[y_train==1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We build our class NB in order to implement the Naive Bayes classifier from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        # init train data vocabulary\n",
    "        self.V = VOCABULARY\n",
    "        \n",
    "        # init length of the train data vocabulary\n",
    "        self.nDistinctWordsInV = len(self.V)\n",
    "        \n",
    "        # init 2 arrays of conditional probability of each word knowing its class \n",
    "        # 2 arrays as we know there are positive and negative classes\n",
    "        self.array_condprob_c_neg = np.ndarray(shape=(self.nDistinctWordsInV), dtype=float)\n",
    "        self.array_condprob_c_pos = np.ndarray(shape=(self.nDistinctWordsInV), dtype=float)\n",
    "        \n",
    "        # init prior probability of each class\n",
    "        self.prior_c_neg = 0\n",
    "        self.prior_c_pos = 0\n",
    "        \n",
    "        # init the highest prior probabilities of the different classes\n",
    "        self.prior_c = 0 \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # set negative and positive texts\n",
    "        self.X = X\n",
    "        self.y = y \n",
    "        \n",
    "        # get the number of the documents of test data : n_samples\n",
    "        # get the number of unique words of the whole test data : n_features\n",
    "        n_samples, n_features = self.X.shape   \n",
    "        \n",
    "        # Compute prior probability of the existing classes\n",
    "        # Here, class 0 for negative critics and class 1 for positive critics\n",
    "        self.prior_c = int(n_samples / 2)           \n",
    "        \n",
    "        X_neg = X[y == 0] #X_[0:self.prior_c]\n",
    "        X_pos = X[y == 1] #X_[-self.prior_c:]\n",
    "        \n",
    "        # Compute total number of words in negative reviews and in positive reviews\n",
    "        nTotal_words_in_class_neg = np.sum(X_neg)\n",
    "        nTotal_words_in_class_pos = np.sum(X_pos)\n",
    "\n",
    "        for word_id in tqdm(range(0, n_features)):\n",
    "            \n",
    "            # compute the conditional probability of each word knowing its class\n",
    "            condprob_word_in_c_neg = (np.sum(X_neg[:, word_id]) + 1) / (nTotal_words_in_class_neg + self.nDistinctWordsInV)\n",
    "            condprob_word_in_c_pos = (np.sum(X_pos[:, word_id]) + 1) / (nTotal_words_in_class_pos + self.nDistinctWordsInV)\n",
    "            \n",
    "            # save the results in arrays\n",
    "            self.array_condprob_c_neg[word_id] = condprob_word_in_c_neg\n",
    "            self.array_condprob_c_pos[word_id] = condprob_word_in_c_pos     \n",
    "                \n",
    "\n",
    "    # Function predict\n",
    "    def predict(self, X):\n",
    "        # set negative and positive texts\n",
    "        n_sample_test , n_features_test = X.shape\n",
    "        \n",
    "        # For each document in the test data\n",
    "        for doc_id in tqdm(range(n_sample_test)):\n",
    "        \n",
    "            # compute the log prior propability of the class knowing all the classes\n",
    "            self.prior_c_neg = math.log(self.prior_c)\n",
    "            self.prior_c_pos = math.log(self.prior_c)\n",
    "            \n",
    "            # get the count of unique words in a given document to predict\n",
    "            Xdoc_test = X[doc_id, :]\n",
    "            \n",
    "            # compute the log prior probability of the classes of the document knowing its vocabulary\n",
    "            for word_id in range(n_features_test):\n",
    "\n",
    "                self.prior_c_neg +=  Xdoc_test[word_id] * math.log(self.array_condprob_c_neg[word_id])\n",
    "                self.prior_c_pos +=  Xdoc_test[word_id] * math.log(self.array_condprob_c_pos[word_id])\n",
    "              \n",
    "                #self.prior_c_neg +=  Xdoc_test[word_id] * math.log(self.vocabulary_condprob_c_neg[word_id])\n",
    "                #self.prior_c_pos +=  Xdoc_test[word_id] * math.log(self.vocabulary_condprob_c_pos[word_id])\n",
    "              \n",
    "            # take the highest log prior probability and assign its corresponding class to the document to predict\n",
    "            if self.prior_c_neg > self.prior_c_pos:\n",
    "                self.y_hat[doc_id] = 0\n",
    "            else : \n",
    "                self.y_hat[doc_id] = 1\n",
    "                \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        # set dimension of predictions variable\n",
    "        self.y_hat = np.ones(len(y), dtype=np.int)\n",
    "        \n",
    "        # do predictions\n",
    "        self.predict(X)\n",
    "        \n",
    "        y = y.astype(int)\n",
    "        score = np.mean(np.isclose(y, self.y_hat))\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the performance of our model with a 5-folds cross-validation and see the accuracy of our classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20360/20360 [00:02<00:00, 8603.07it/s]\n",
      "100%|██████████| 1733/1733 [04:22<00:00,  7.64it/s]\n",
      "100%|██████████| 20360/20360 [00:02<00:00, 10058.98it/s]\n",
      "100%|██████████| 1733/1733 [03:51<00:00,  7.74it/s]\n",
      "100%|██████████| 20360/20360 [00:02<00:00, 8145.12it/s]\n",
      "100%|██████████| 1733/1733 [03:53<00:00,  7.64it/s]\n",
      "100%|██████████| 20360/20360 [00:01<00:00, 10391.41it/s]\n",
      "100%|██████████| 1732/1732 [03:51<00:00,  7.88it/s]\n",
      "100%|██████████| 20360/20360 [00:02<00:00, 10017.34it/s]\n",
      "100%|██████████| 1731/1731 [04:06<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without stop words, the scores with cross validation in 5 splits are : [ 0.76457011  0.76283901  0.76399308  0.76847575  0.76487579]\n",
      "==========================================================================================\n",
      "Mean score without stop words =  0.764950747529\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation with training data:\n",
    "#   - X : matrix of words occurrences per review\n",
    "#   - y : labels (0 for negative  , 1 for positive)\n",
    "\n",
    "nb = NB()\n",
    "n_fold_5 = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "scores = cross_val_score(nb, X_train, y_train, cv=n_fold_5)\n",
    "print('Without stop words, the scores with cross validation in 5 splits are : ' + str(scores))\n",
    "print(\"==========================================================================================\")\n",
    "mean_scores_cv_ = np.mean(scores)\n",
    "print(\"Mean score without stop words = \", mean_scores_cv_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Let's fit on the whole trainining set and score on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20360/20360 [00:03<00:00, 6343.59it/s]\n",
      "100%|██████████| 2000/2000 [04:52<00:00,  7.48it/s]\n"
     ]
    }
   ],
   "source": [
    "nb = NB()\n",
    "nb.fit(X_train, y_train)\n",
    "score_test = nb.score(X_test, y_test)*100\n",
    "#print('Without Stop words, the score for the test is %.4f%%'%()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.549999999999997"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "## Scikit-learn and NLTK  implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We perform a cross validation, taking into account the stop words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 482 ms, sys: 37.6 ms, total: 519 ms\n",
      "Wall time: 553 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Prepare the data with stop words and punctuation\n",
    "all_reviews_skl = negative_reviews + positive_reviews\n",
    "train_in_skl = list(pd.Series(all_reviews_skl).loc[rand_index[2000:]].values)\n",
    "test_in_skl =  list(pd.Series(all_reviews_skl).loc[rand_index[:2000]].values)\n",
    "\n",
    "# Vectorize the data\n",
    "counts_vecto = CountVectorizer()\n",
    "counts_vecto.fit(train_in_skl)\n",
    "X_train_skl = counts_vecto.transform(train_in_skl)\n",
    "X_test_skl = counts_vecto.transform(test_in_skl)\n",
    "\n",
    "# Score and predict\n",
    "nb_skl = MultinomialNB()\n",
    "scores_skl = cross_val_score(nb_skl, X_train_skl, y_train, cv=n_fold_5)\n",
    "skl_scores_cv_ =  np.mean(scores_skl)\n",
    "\n",
    "nb_skl2 = MultinomialNB()\n",
    "nb_skl2.fit(X_train_skl, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary with punctuation and stop words:  16641\n",
      "Mean CV score in 76.703% ,Test Score 78.650%\n",
      "Size of initial vocabulary :  16641\n"
     ]
    }
   ],
   "source": [
    "# get vocabulary keys root\n",
    "print('Size of vocabulary with punctuation and stop words: ', len(counts_vecto.vocabulary_.keys()))\n",
    "print('Mean CV score in %.3f%% ,Test Score %.3f%%'%(skl_scores_cv_*100, nb_skl2.score(X_test_skl, y_test)*100))\n",
    "# get vocabulary keys root\n",
    "roots_ = counts_vecto.vocabulary_.keys()\n",
    "print('Size of initial vocabulary : ', len(roots_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**scikit-learn** classifier goes much faster than the manual ones as it is optimized\n",
    "### Repeat the tasks without stop words and punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CV score 75.987% , Test Score 77.050%\n",
      "Size of vocabulary without stop words:  16325\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Vectorize the data\n",
    "counts_vecto = CountVectorizer(strip_accents='ascii', stop_words='english', analyzer='word')\n",
    "counts_vecto.fit(train_in_skl)\n",
    "X_train_skl = counts_vecto.transform(train_in_skl)\n",
    "X_test_skl = counts_vecto.transform(test_in_skl)\n",
    "\n",
    "# Score and predict\n",
    "nb_skl = MultinomialNB()\n",
    "scores_skl = cross_val_score(nb_skl, X_train_skl, y_train, cv=n_fold_5)\n",
    "skl_scores_cv_ =  np.mean(scores_skl)\n",
    "\n",
    "nb_skl2 = MultinomialNB()\n",
    "nb_skl2.fit(X_train_skl, y_train)\n",
    "\n",
    "print('Mean CV score %.3f%% , Test Score %.3f%%'%( skl_scores_cv_*100, nb_skl2.score(X_test_skl, y_test)*100))\n",
    "# get vocabulary keys root\n",
    "roots_ = counts_vecto.vocabulary_.keys()\n",
    "print('Size of vocabulary without stop words: ', len(roots_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use NLTK to do a stemming with the class *SnowballStemmer*.\n",
    "we keep the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CV score 76.518% , Test Score 78.800%\n",
      "Size of vocabulary with stemming and without stop words:  13297\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import SnowballStemmer\n",
    "\n",
    "def GetWordsRoots(text):\n",
    "    \n",
    "    wordsList = text.split()\n",
    "    for word in wordsList:\n",
    "        yield(SnowballStemmer(\"english\", ignore_stopwords=False).stem(word))\n",
    "    \n",
    "counts_vecto_stem = CountVectorizer(analyzer = GetWordsRoots).fit(train_in_skl)\n",
    "X_train_skl = counts_vecto_stem.transform(train_in_skl)\n",
    "X_test_skl = counts_vecto_stem.transform(test_in_skl)\n",
    "\n",
    "# Score and predict\n",
    "nb_skl = MultinomialNB()\n",
    "scores_skl = cross_val_score(nb_skl, X_train_skl, y_train, cv=n_fold_5)\n",
    "skl_scores_cv_ =  np.mean(scores_skl)\n",
    "\n",
    "nb_skl2 = MultinomialNB()\n",
    "nb_skl2.fit(X_train_skl, y_train)\n",
    "\n",
    "print('Mean CV score %.3f%% , Test Score %.3f%%'%( skl_scores_cv_*100, nb_skl2.score(X_test_skl, y_test)*100))\n",
    "# Vectorize input data and fit\n",
    "\n",
    "# get vocabulary keys root\n",
    "roots_ = counts_vecto_stem.vocabulary_.keys()\n",
    "print('Size of vocabulary with stemming and without stop words: ', len(roots_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming reduces the number of words . We have 13297 instead of 16325 and the score has improved. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We filter by grammatical category (POS: Part Of Speech) and keep only the names, verbs, adverbs and adjectives for the classification\n",
    "\n",
    "#### http://www.nltk.org/book/ch05.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB: Mean CV score 76.114% , Test Score 77.500%\n",
      "LR: Mean CV score 76.114% , Test Score 75.950%\n",
      "Size of vocabulary with POS:  16641\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "#import nltk\n",
    "#nltk.download()\n",
    "\n",
    "def GetSignificantWords(texts):\n",
    "    words_and_tags = nltk.pos_tag(nltk.word_tokenize(texts))\n",
    "    for word in words_and_tags:\n",
    "        if word[1] in ['JJ', 'JJR', 'JJS','NN', 'NNS', 'NNP', 'NNPS','RB', 'RBR','RBS',\\\n",
    "                        'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "            yield word[0]\n",
    "\n",
    "# Vectorize input data and fit\n",
    "counts_vecto_pos = CountVectorizer(analyzer = GetSignificantWords).fit(train_in_skl)\n",
    "X_train_skl = counts_vecto_pos.transform(train_in_skl)\n",
    "X_test_skl = counts_vecto_pos.transform(test_in_skl)\n",
    "\n",
    "# Score and predict\n",
    "nb_skl = MultinomialNB()\n",
    "scores_skl = cross_val_score(nb_skl, X_train_skl, y_train, cv=n_fold_5)\n",
    "skl_scores_cv_ =  np.mean(scores_skl)\n",
    "\n",
    "nb_skl2 = MultinomialNB()\n",
    "nb_skl2.fit(X_train_skl, y_train)\n",
    "\n",
    "print('NB: Mean CV score %.3f%% , Test Score %.3f%%'%( skl_scores_cv_*100, nb_skl2.score(X_test_skl, y_test)*100))\n",
    "\n",
    "# Vectorize input data and fit\n",
    "\n",
    "# get vocabulary keys root\n",
    "significant_words = counts_vecto.vocabulary_.keys()\n",
    "print('Size of vocabulary with POS: ', len(significant_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** POS reduces the number of tokens but less than Stemming . We have 38733 instead of 39443. The score is lower than stemming score and than manual score.**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
